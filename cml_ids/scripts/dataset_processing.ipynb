{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import os, glob\n",
    "import copy\n",
    "\n",
    "\n",
    "from utils.dataset_processing.dataset_processing import DatasetPreprocess\n",
    "from utils.dataset_processing.pcap_to_flow import Pcap2Flow\n",
    "from utils.dataset_processing.feature_selection import FeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Setup Pathes, Parameters, Objects #########################\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "############################# Setup Parameters #############################\n",
    "# number of packets extracted within a flow\n",
    "packet_count = 8\n",
    "\n",
    "\n",
    "############################### Setup Pathes ###############################\n",
    "# find all pcap pathes\n",
    "dataset_path = \"../../../datasets/pcaps\"\n",
    "pcap_path_list = glob.glob(os.path.join(dataset_path, f\"*-WorkingHours.pcap\"))\n",
    "\n",
    "# setup path list for flow-based datasets\n",
    "raw_flow_path_list = []\n",
    "raw_flow_base_path = \"../../dataset/raw_flow_datasets/\"\n",
    "flow_preprocessed_path_list = []\n",
    "flow_preprocessed_single_base_path = \"../../dataset/flow_preprocessed_datasets/singal_datasets/\"\n",
    "flow_preprocessed_merged_path = \"../../dataset/flow_preprocessed_datasets/merged_datasets/\" + f\"flow_preprocessed_merged_{packet_count}.csv\"\n",
    "flow_preprocessed_merged_balanced_path = flow_preprocessed_merged_path[: -4] + \"_balanced.csv\"\n",
    "\n",
    "for pcap_path in pcap_path_list:\n",
    "    # find the dataset day\n",
    "    left_matching = \"pcaps/\"\n",
    "    right_matching = \"-Working\"\n",
    "    dataset_day = pcap_path[pcap_path.index(left_matching) + len(left_matching): pcap_path.index(right_matching)]\n",
    "    raw_flow_path = raw_flow_base_path +  f\"{dataset_day}_raw_flow_{packet_count}.csv\"\n",
    "    raw_flow_path_list.append(raw_flow_path)\n",
    "    flow_preprocessed_path = flow_preprocessed_single_base_path + f\"{dataset_day}_flow_preprocessed_{packet_count}.csv\"\n",
    "    flow_preprocessed_path_list.append(flow_preprocessed_path)\n",
    "    \n",
    "\n",
    "############################ Initialize Objects ############################\n",
    "pcap2Flow = Pcap2Flow()\n",
    "dataProc = DatasetPreprocess()\n",
    "featureSelection = FeatureSelection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Dataset Preprocessing ##########################\n",
    "############################################################################\n",
    "\n",
    "############################# Unused Features ##############################\n",
    "# identification features\n",
    "features_identificaiton = ['id', \n",
    "                           'expiration_id', \n",
    "                           'src_ip', \n",
    "                           'src_mac', \n",
    "                           'src_oui', \n",
    "                           'src_port',\n",
    "                           'dst_ip', \n",
    "                           'dst_mac', \n",
    "                           'dst_oui', \n",
    "                           'dst_port', \n",
    "                           'ip_version', \n",
    "                           'vlan_id', \n",
    "                           'tunnel_id']\n",
    "# features related 'mean' except 'bidirectional_mean_ps'\n",
    "features_mean = ['src2dst_mean_ps', 'dst2src_mean_ps']\n",
    "\n",
    "# sum of dropped features\n",
    "features_drop_sum = features_identificaiton + features_mean\n",
    "\n",
    "# wildcards\n",
    "feature_wildcards = ['ms', 'protocol', 'stddev']\n",
    "\n",
    "\n",
    "############### Aggregation, Preprocessing (singal dateset) ################\n",
    "# aggregate packet-based dataset (.pcap) into flow-based dataset (.csv)\n",
    "for i, path in enumerate(pcap_path_list):\n",
    "    pcap2Flow.to_flow(pcap_path=path, save_path=raw_flow_path_list[i],\n",
    "                       flow_extract_type='sub-flow', limit=packet_count)\n",
    "\n",
    "# label flow entry and remove unused features\n",
    "for i, path in enumerate(raw_flow_path_list):\n",
    "    dataProc.preprocess(dataset_path=path, save_path=flow_preprocessed_path_list[i], packet_count=packet_count, drop_features=features_drop_sum, drop_features_wildcards=feature_wildcards)\n",
    "\n",
    "############################### Merge Datasets ##############################\n",
    "dataProc.merge(dataset_path_list=flow_preprocessed_path_list, save_path=flow_preprocessed_merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Dataset Balancing ############################\n",
    "############################################################################\n",
    "df_balanced = dataProc.balance_dataset(dataset_path=flow_preprocessed_merged_path, save_path=flow_preprocessed_merged_balanced_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Feature Selection #############################\n",
    "#############################################################################\n",
    "# pathes\n",
    "feature_importance_path = \"../base_files/feature_importances.csv\"\n",
    "logging_path = \"../../logs/computation_feature_importance_logger.log\"\n",
    "logging_info = \"Dataset: 8 pkts of sub-flow, balanced\"\n",
    "\n",
    "\n",
    "featureSelection.compute_feature_importances(dataset_path=flow_preprocessed_merged_balanced_path, feature_scores_save_path=feature_importance_path, logging_path=logging_path, logging_info=logging_info, repeat_time=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Refine Dataset ##############################\n",
    "#############################################################################\n",
    "feature_num = 20\n",
    "feature_importance_path = \"../base_files/feature_importances.csv\"\n",
    "\n",
    "dataset_refined_save_path = flow_preprocessed_merged_balanced_path[: -4] + f\"_feature_num_{feature_num}.csv\"\n",
    "\n",
    "# refine all datasets\n",
    "featureSelection.refine_dataset(feature_scores_path=feature_importance_path, dataset_path=flow_preprocessed_merged_balanced_path, dataset_relevant_save_path=dataset_refined_save_path, relevant_features_num=feature_num)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
